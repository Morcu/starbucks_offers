{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset = pd.read_pickle(\"./x_dataset.pkl\")\n",
    "y_dataset = pd.read_pickle(\"./y_dataset.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_dataset[\"id\"]\n",
    "del y_dataset[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataset = y_dataset.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ae264e3637204a6fb9bb56bc8210ddfd    float64\n",
       "4d5c57ea9a6940dd891ad53e9dbe8da0    float64\n",
       "3f207df678b143eea3cee63160fa8bed    float64\n",
       "9b98b8c7a33c4b65b9aebfe6a799e6d9    float64\n",
       "0b1e1539f2cc45b7b9fa7c272da2e1d7    float64\n",
       "2298d6c36e964ae4a3e7e9706d1fb8c2    float64\n",
       "fafdcd668e3743c1bb461111dcafc2a4    float64\n",
       "5a8bc65990b245e5a138643cd4eb9837    float64\n",
       "f19421c1d4aa40978ebb69ca19b0e20d    float64\n",
       "2906b810c7d4411798c6938adc9daaa5    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x_dataset, y_dataset, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_features, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up linear layers.\n",
    "        Use the input parameters to help define the layers of your model.\n",
    "        :param input_features: the number of input features in your training/test data\n",
    "        :param hidden_dim: helps define the number of nodes in the hidden layer(s)\n",
    "        :param output_dim: the number of outputs you want to produce\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Fully conected\n",
    "        self.fc1 = nn.Linear(input_features, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
    "       \n",
    "         # Droput \n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        # Softmax Layer\n",
    "        self.soft = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on input features, x.\n",
    "        :param x: A batch of input features of size (batch_size, input_features)\n",
    "        :return: A single, sigmoid-activated value as output\n",
    "        \"\"\"\n",
    "        \n",
    "        # feedforward behavior\n",
    "        \n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return self.soft(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(xTrain.shape[1], 16, 32, yTrain.shape[1]).to(device)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion= nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7269590519171764\n",
      "Training loss: 0.7266393195876225\n",
      "Training loss: 0.7265921910266977\n",
      "Training loss: 0.7265339942221624\n",
      "Training loss: 0.7265582475040683\n",
      "Training loss: 0.7265190638626722\n",
      "Training loss: 0.7265372266689203\n",
      "Training loss: 0.7265143095030331\n",
      "Training loss: 0.7265213423907252\n",
      "Training loss: 0.726529092702399\n",
      "Training loss: 0.7265283122613779\n",
      "Training loss: 0.7265332564785395\n",
      "Training loss: 0.726546058688633\n",
      "Training loss: 0.7265425674086434\n",
      "Training loss: 0.7264847116571415\n",
      "Training loss: 0.7264839995037864\n",
      "Training loss: 0.7264834834031135\n",
      "Training loss: 0.72651325330455\n",
      "Training loss: 0.7265239917088993\n",
      "Training loss: 0.7265337550720186\n",
      "Training loss: 0.7265005472850116\n",
      "Training loss: 0.7265036763469175\n",
      "Training loss: 0.7264763486384143\n",
      "Training loss: 0.7265022538385308\n",
      "Training loss: 0.7264920685025649\n",
      "Training loss: 0.7264948570252314\n",
      "Training loss: 0.7265226235453008\n",
      "Training loss: 0.7265237361428586\n",
      "Training loss: 0.7265060515234072\n",
      "Training loss: 0.7265006122353211\n",
      "Training loss: 0.7264991885775246\n",
      "Training loss: 0.7265132732538423\n",
      "Training loss: 0.7264897657703646\n",
      "Training loss: 0.726496613438433\n",
      "Training loss: 0.7264898695388438\n",
      "Training loss: 0.7265016641410269\n",
      "Training loss: 0.7265048619648888\n",
      "Training loss: 0.7265013880798883\n",
      "Training loss: 0.7265024685632131\n",
      "Training loss: 0.7264891764196836\n",
      "Training loss: 0.7265329653202641\n",
      "Training loss: 0.7265108191057728\n",
      "Training loss: 0.7265021931816186\n",
      "Training loss: 0.7265012806203209\n",
      "Training loss: 0.7264841602353256\n",
      "Training loss: 0.7264996433194625\n",
      "Training loss: 0.7264839183867963\n",
      "Training loss: 0.7264630027018136\n",
      "Training loss: 0.7264722582712276\n",
      "Training loss: 0.7264758701504999\n",
      "CPU times: user 7min 23s, sys: 36.4 s, total: 8min\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 50\n",
    "model = model.double()\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for x, y in zip(xTrain.values, yTrain.values):\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        y = torch.from_numpy(y).to(device)\n",
    "        log_ps = model(x)\n",
    "\n",
    "        loss = criterion(log_ps, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(xTrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
